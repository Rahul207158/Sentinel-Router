version: '3.8'

services:
  sentinel:
    build: .
    #container_name: sentinel-router
    ports:
      - "8000:8000"
    environment:
      - OPENAI_API_KEY=${OPENAI_API_KEY}
      # This special URL allows the container to talk to Ollama running on your Mac
      - LOCAL_LLM_URL=http://host.docker.internal:11434
    extra_hosts:
      - "host.docker.internal:host-gateway"
      # volumes:
      #   # Mount the model folder so you don't have to rebuild docker when models change
      #   - ./sentinel_router_v1:/app/sentinel_router_v1

      # Optional: You can run Ollama inside docker too, 
      # but for V1 let's stick to your Mac's installed Ollama for GPU speed.
